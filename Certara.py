import time
import os
import pandas as pd
import mrmr
from sklearn.ensemble import AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import recall_score, precision_score, accuracy_score
import plotext as plt
from joblib import dump
from imblearn.over_sampling import SMOTEN
import numpy as np


pathtofile = os.environ['PATH_TO_FILE']
maxfeatures = int(os.environ['MAX_FEATURES'])
usesmote = (os.environ['USE_SMOTE'] == "True")

df = pd.read_csv(pathtofile)

breakdown = df.fillna(2).apply(pd.Series.value_counts)

for column in breakdown:
    target = breakdown[column]
    total = target.sum(axis=0)

    if column == "Result":
        positives = target[1]
        negatives = target[0]

    for i in target.index:
        if target[i]/total < 0.001:
            print(column + " is extremely skewed")
        elif target[i]/total < 0.01:
            print(column + " is skewed")
        elif i == 2 and target[i]/total > 0.05:
            print(column + " is nulled a lot")


print(" ")
print("Total Rows:")
print(total)
print("Positive Examples:")
print(positives)
print("Negative Examples:")
print(negatives)

# This gets a priority list of all the features with K=86 (number of features in the dataset)
selected_features = mrmr.mrmr_classif(X=df.drop(['Result'], axis=1), y=df['Result'], K=86)

x_possible, x_test, y_possible, y_test = train_test_split(df.drop(['Result'], axis =1), df['Result'],
                                                    test_size=0.13,
                                                    random_state=0,
                                                    stratify=df['Result'])

if not usesmote:
    x_train, y_train = x_possible, y_possible
else:
    x_train, y_train = SMOTEN(random_state=0).fit_resample(x_possible, y_possible)
    smote_breakdown = pd.Series.value_counts(y_train)
    print("SMOTE-d Positives:")
    print(smote_breakdown[1])
    print("SMOTE-d Negatives:")
    print(smote_breakdown[0])

print("Going to training")


def calcFBeta(prec, recall, beta):
    betatoo = np.power(beta, 2)
    return (1+betatoo)*(prec*recall/((betatoo*prec)+recall))


def runSweep(model_to_test, parameter_space, prefixx, maxF):
    accuracy_df = pd.DataFrame(columns=['Number of Features', 'Accuracy'])
    recall_df = pd.DataFrame(columns=['Number of Features', 'Accuracy'])
    precision_df = pd.DataFrame(columns=['Number of Features', 'Accuracy'])

    best_score = 0.0
    best_model = ""

    os.mkdir(prefixx)
    for n in range(1, maxF):
        subset_features = selected_features[0:n]
        model_to_test.fit(x_train[subset_features], y_train)
        y_pred = model_to_test.predict(x_test[subset_features])
        recall_scoree = recall_score(y_test, y_pred)
        prec_scoree = precision_score(y_test, y_pred)
        # calc an F_beta score where recall is 3x more important than precision
        fb = calcFBeta(prec_scoree, recall_scoree, 3)
        recall_df = pd.concat([recall_df, pd.DataFrame([{'Number of Features': n, 'Recall': recall_scoree}])], axis=0, ignore_index=True)
        precision_df = pd.concat([precision_df, pd.DataFrame([{'Number of Features': n, 'Precision': prec_scoree}])], axis=0, ignore_index=True)
        accuracy_df = pd.concat([accuracy_df, pd.DataFrame([{'Number of Features': n, 'Accuracy': accuracy_score(y_test, y_pred)}])], axis=0, ignore_index=True)
        filenamee = prefixx+"/feature"+str(n)
        dump(model_to_test, filenamee)
        if fb > best_score:
            best_score = fb
            best_model = filenamee

    plt.plot(accuracy_df['Number of Features'], accuracy_df['Accuracy'])
    plt.title('Accuracy vs. Number of Features')
    plt.xlabel('Number of Features')
    plt.ylabel('Accuracy')
    plt.grid(True)
    plt.xlim(0, maxF)
    plt.show()
    plt.clear_data()
    plt.plot(precision_df['Number of Features'], precision_df['Precision'])
    plt.title('Precision vs. Number of Features')
    plt.xlabel('Number of Features')
    plt.ylabel('Precision')
    plt.grid(True)
    plt.xlim(0, maxF)
    plt.show()
    plt.clear_data()
    plt.plot(recall_df['Number of Features'], recall_df['Recall'])
    plt.title('Recall vs. Number of Features')
    plt.xlabel('Number of Features')
    plt.ylabel('Recall')
    plt.grid(True)
    plt.xlim(0, maxF)
    plt.show()
    plt.clear_data()

    print("The best model is saved at '"+best_model+"' and has a score of "+str(best_score))


dirname = os.path.dirname(__file__)
fileprefix = os.path.join(dirname, "/models/")
os.mkdir(fileprefix)

# params for a really obtuse search
nnParams = {'hidden_layer_sizes': [(10, 30, 10), (20,)], 'alpha': [0.0001], 'learning_rate_init': [0.05, 0.001]}
# params for just a single huge nn
# nnParams = {'hidden_layer_sizes': [(100, 100)], 'alpha': [0.0001], 'learning_rate_init': [0.05]}
runSweep(GridSearchCV(MLPClassifier(), nnParams, cv=5, scoring='recall', n_jobs=-1), nnParams, fileprefix+"nnModel", maxfeatures)

# adaboost gets the whole feature sweep but no param sweep to save time
adaParams = {}
runSweep(AdaBoostClassifier(n_estimators=200, learning_rate=1), adaParams, fileprefix+"adaModel", 86)

# copy trained models to volume
os.system('cp -r '+fileprefix+'/* /tmp/')

#just in case.....
time.sleep(1)
quit()
