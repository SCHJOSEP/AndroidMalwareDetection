# Malware Detection PoC
Run 'docker volume create [volume-name]' to make a place to store the trained models. \
Run 'docker build -t [imagename] .' to build the project. \
Run 'docker run -v [volume-name]:/tmp/ --env-file Docker.env [imagename]' to train models and get results. \

Please also note that you must provide the file or the path to the csv with the data. \

This project uses mRMR to do feature selection because the data is so small I can efficiently just 
make a ranked list of all 86 of the features. mRMR ranks features based on maximum relevancy with minimal redundancy.
Performance of models is tested across a range of features set by the env variable MAX_FEATURES.
This project also evaluates SMOTE-N, a technique to create synthetic
data points to compensate for the low ratio of positive malware examples. Oversampling can be turned on and off
by setting the env variable USE_SMOTE. \

The main entry point is the function runSweep which tests a model across a range of parameters and a range
of features. It saves the models in each test. It is set up to evaluate a neural net using grid search cross
validation and the AdaBoost algorithm with decision trees. \

I have heard that I can include even more techniques to deal with a low positive signal and may expect 
better results. I could also experiment with different settings for the SMOTE algorithm. 
I would love to implement an integration for custom cost functions or at least push the cross validation scoring
a little bit in the direction of precision with an F_Beta score where the beta roughly parametrizes how much 
more important recall is than precision. I also did not explore the hyperparamter 
space of the models evaluated very deeply. Better machines would also be nice to test on. 